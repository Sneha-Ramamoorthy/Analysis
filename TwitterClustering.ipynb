{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glitter127/Analysis/blob/main/TwitterClustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy\n"
      ],
      "metadata": {
        "id": "8y87I8c554IX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55faafe6-7614-40dc-967b-e8ae3c1d1d79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2EyVNxOl5lI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1651bbb2-320f-4658-d554-f87b0127b38e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication failed: 401 Unauthorized\n",
            "89 - Invalid or expired token.\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "# Add your credentials here\n",
        "api_key = \"your_api_key\"\n",
        "api_key_secret = \"your_api_key_secret\"\n",
        "access_token = \"your_access_token\"\n",
        "access_token_secret = \"your_access_token_secret\"\n",
        "BEARER_TOKEN = \"your_BEARER_TOKEN\"\n",
        "\n",
        "# Authenticate\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# Verify authentication\n",
        "try:\n",
        "    user = api.verify_credentials()\n",
        "    print(f\"Authenticated as {user.screen_name}\")\n",
        "except Exception as e:\n",
        "    print(\"Authentication failed:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "\n",
        "# Authenticate using Twitter API v2\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Fetch live tweets using v2 API\n",
        "def fetch_tweets_v2(query, count=10):\n",
        "    tweets = []\n",
        "    response = client.search_recent_tweets(query=query, max_results=count, tweet_fields=[\"text\"])\n",
        "    for tweet in response.data:\n",
        "        tweets.append(tweet.text)\n",
        "    return tweets\n",
        "\n",
        "# Fetch tweets\n",
        "tweets = fetch_tweets_v2(\"Python\", count=10)\n",
        "print(tweets)\n"
      ],
      "metadata": {
        "id": "f7NTk5e863Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Replace with your own Twitter API credentials\n",
        "BEARER_TOKEN = BEARER_TOKEN\n",
        "\n",
        "# Authenticate with Twitter API v2\n",
        "client = tweepy.Client(bearer_token=BEARER_TOKEN)\n",
        "\n",
        "# Function to fetch live tweets\n",
        "def fetch_tweets_v2(query, count=10):\n",
        "    tweets = []\n",
        "    response = client.search_recent_tweets(query=query, max_results=count, tweet_fields=[\"text\"])\n",
        "    if response.data:\n",
        "        for tweet in response.data:\n",
        "            tweets.append(tweet.text)\n",
        "    return tweets\n",
        "\n",
        "# Function to clean tweets\n",
        "def clean_tweet(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r\"\\@\\w+|\\#\", \"\", text)  # Remove mentions and hashtags\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Fetch and clean tweets\n",
        "tweets = fetch_tweets_v2(\"Python\", count=10)\n",
        "cleaned_tweets = [clean_tweet(tweet) for tweet in tweets]\n",
        "\n",
        "# Convert text into numerical form using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(cleaned_tweets)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "num_clusters = 3  # Choose the number of clusters\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assign cluster labels to tweets\n",
        "df = pd.DataFrame({\"Tweet\": cleaned_tweets, \"Cluster\": kmeans.labels_})\n",
        "\n",
        "# Print clustered tweets\n",
        "for i in range(num_clusters):\n",
        "    print(f\"\\nðŸ”¹ Cluster {i}:\")\n",
        "    print(df[df[\"Cluster\"] == i][\"Tweet\"].tolist())\n"
      ],
      "metadata": {
        "id": "0gNYEauA7Pfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}